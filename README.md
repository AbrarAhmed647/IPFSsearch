# IPFSFinder
IPFSFinder is a centralized search engine for decentralized peer-peer Inter-Planetary File System(IPFS) that uses KEYWORDS that are easy for humans to remember instead of CID (the traditional way of searching through IPFS network) to search through the IPFS network for a file

__Drawbacks of searching across the network using CID only:__
1. Can only search for content with known CID (its like using ipaddresses instead of URLs, which is bad)
2. Not suitable for all application scenarios
3. Requires hosting an IPFS node
4. Slower data manipulation and latency in data retrieval because the search is decentralized


Why choose IPFS?- We chose IPFS because it has more extensive documentation and a larger user base

Our solution which is built from scratch eliminates all the mentioned drawbacks and leverages the centralization search advantages like GoogleSE. 

## Motivation and objectives
With the development of computer technology, the scale of data is also increasing, and the decentralized data storage method can meet the data storage needs in the big data environment. The Interplanetary File System[1] created a peer-to-peer (P2P) distributed file system, upgraded the existing network structure, and realized decentralized storage.
For each file uploaded to the IPFS system, the system will return a unique file identifier CID(content identifier), but the resource requester can download the corresponding file in IPFS only by providing the CID accurately. IPFS only supports CID-based data acquisition methods, which restricts its application. Due to the lack of corresponding search functions, it is difficult for resource requesters to obtain relevant data through keywords or other descriptive information. Studying the data acquisition method of IPFS can help users search for data according to their own needs, which is conducive to the sharing and discovery of IPFS data, and makes IPFS meet more application scenarios. In addition, when the file identifier is lost or forgotten, the original data can be obtained in other ways to avoid data "lost connection". Even with trying to locate files by CID, the speed at which these queries get served does not compare with the speed provided by Google, Bing, or other centralized search engines.
In order to solve the above problems, the objective of this project is to build a centralized search engine on top of IPFS to solve the search problem. In this project, every node needs to start a software for generating its local index and at the same time a centralized server that receives all the local indexes sent by participating nodes and combines them to create a global index. End user will send a query to the centralized server directly instead of sending a query to node for searching specific files. Beside designing a centralized search engine on IPFS, this project will also focus on: 

1. Design a strategy for global-local sync, implement an efficient way of maintaining the global index up to date.

2. Build a strategy for content moderation based on characteristics of the file, identify illegal files and exclude them from the search engine system.
[1] BENET J.IPFS-content addressed，versioned，p2p file system[J].arXiv:1407.3561， 2014.

## Related Work
In recent times, there has been much active research on IPFS due to the advantages of this decentralized file system over traditional file storing and retrieving systems. Moreover, similar to the functionality of search engines of the web that are used to search the internet by content, IPFS lacks a system or search engine that uses keyword searches to locate the files that are stored with CIDs as indexes. Also, this search engine should be able to allow the user to search using the keywords of the content instead of memorizing CIDs. The authors in [1] proposed a system that relates the keywords typed by user with the list of unique CIDs of the IPFS to retrieve the files that correspond to the keywords. But this search engine is decentralized and uses DHT and Bloom filters for the implementation that induces latency in response time of the system. Another decentralized search engine as proposed in [2], uses a sniffer that pulls hashes between the nodes and crawler that extracts the content from the hashes and indexes them. The search engine architecture sniffs DHT gossip of the file exchanges and implements an indexing scheme. Moreover, the system captures metadata of the content and builds indexes. The main disadvantage of this approach is that the system is not scalable and not fast, which are main challenges of the file systems.
A query based approach is proposed by the authors in [3], where if a node wants to search a content it publishes a query to the network and the peers respond to the query with a summary file. Every node maintains a summary of the content that it hosts in a file format and whenever a search query is received it tries to match with its local summary file and return the response summary file with a match score. The node that requested the data maintains a local summary file in cache that has the summary of the data of the node that has the content and its matching score. However, the problem is the entire network knows the whereabouts of the keyword in the network and the security of the content is compromised since no hashing techniques are used.
In paper [4], the authors propose a CASearch system, a keyword search capability in Content Addressable Network and experiment on IPFS to search content of the files. The search extracts keywords from the available files in the node, hash it and maintains an proactive index of these hash values. Moreover, the search engine ranks the results by computing the distance function between the nodes. However, the search system is decentralized and scalability is an issue.
In this paper we try to address the limitations of these works by proposing a centralized search engine system which will provide better architecture to index the content of the files and be scalable.

## Proposed Work
Our proposed system would work as follows. A client not part of the P2P system (IPFS), wants to search for information stored in IPFS to answer a particular question. To replicate this behavior on the web, a client would navigate to a search engine and type in a query to locate files by content. In IPFS, however, locating files by content is still not possible. To circumvent this limitation, the client would navigate to our search engine to type in their query to receive a list of relevant IPFS-hosted files. Our server can provide a list of the most relevant IPFS files because it maintains a global index of many files stored on many nodes of the IPFS network. It periodically updates this global index with the local index of each node participating in our system. These nodes will be running software to create a local index of the files they currently store and want to make public. These nodes will send their updated local index to our server at certain intervals. This way, the global index is updated to always know in what node a particular file resides. The client then clicks on one of the links provided by our server and gets connected to the IPFS node hosting that file. The node proceeds to send the file directly to the client's computer. This way, the server maintains a global index to direct clients to the appropriate IPFS nodes without worrying about serving files. Below is a diagram that contains the different parts of this system and how they interact with each other:
For the project, our proposed work will consist of five main parts: local index generation, global index generation, local-global index sync method, content moderation, and a front-end access point. We will also include a future works section in the final report outlining the following steps to implement this centralized engine successfully.
 
Local Index Generation: The local index generation part consists of software that will run on the computers of the nodes that decide to participate in our system. The objective of this software is two-fold. It will first build an index of all the files a particular node is hosting and wants to make public. The index will be an inverted index that builds a list of (keyword, file) for all keywords and all files intended to be indexed. After building this index, the software will send it to the central server and the node's ID. The rate at which the software sends the local index to the central server falls within the scope of the local-global index sync section.
Global Index Generation: The global index generation consists of a centralized server that receives all the local indexes sent by participating nodes and combines them to create a global index. This global index will also be an inverted index that creates a list of (keyword, (file, node ID)) for all the files and keywords sent by the nodes. Ranking the files in this index will not be part of our project's scope but will be included in the future works section.

Local-Global Index Sync: The local-global index sync part will devise a strategy for periodically updating the global index with changes in local indexes. The global index must reflect the reality of the IPFS network. Otherwise, the client may have a degraded user experience as some of the files returned by the search engine will not be accessible. On the other hand, constantly forcing nodes to send the status of their local index will put too much pressure on the nodes and the central server's computing resources. This section aims to implement an efficient way of maintaining the global index up to date.

Content Moderation: The content moderation part will include devising a strategy to determine whether files are legal or illegal. Because we would be facilitating the propagation and distribution of files, we need to identify illegal files and not include them in the global index.
Front-End Access Point: The front-end access point will be the entry point to test and use our system. It will be a web page that lets a client type in a query and see the relevant IPFS-hosted files. The scope of our work does not include adding the capabilities for a node to send the file to the client through an off-chain connection. However, the future works section will include the mechanism for this off-chain connection.

## Plan of Action
Local Index Generation: The local index stores all the key-value information on an IPFS node. There are some IPFS libraries that we can refer to. The platform used will be IPFS. We may use Go, Python languages.
Global Index Generation: The global index will host all the key-value information gathered by the server. The server will perform HTTP requests to receive updates from nodes. We may use Go, Python languages.
Local-Global Index Sync: We will research first to find a mechanism for this functionality. This can be periodic updating from the node or update based on cache misses at the server. The platform and software used at this part will be the same as above.
Content Moderation: We will do research about the legal restrictions on file contents first, then look for keywords or use other mechanisms to filter out the files that contain illegal contents. We can either build a content filter between nodes and the server for this checking and filtering purpose, or we can integrate this in the server. The platform and software used will be the same as above.
Front-end Access Point: a website developed using html, css, and javascript that serves and the user interface for searching files. This should be done after the previous steps. The platform will be VSCode for development, Ubuntu VM and Apache for hosting the website.

## Evaluation and Testing Method
Functional testing:
We will perform user tests from the front-end access point and compare results with the ground truth (files stored in IPFS nodes). This includes testing the local index generation, global index generation, local-global index sync, content moderation, and front-end access point. We will create test cases to evaluate the functionality of each component and make sure that they work seamlessly together.

Performance testing:
Statistic analysis will include the missed percentage, error result percentage, and matching percentage. Analysis can be done using Python or Excel. We will simulate different scenarios to test the system's scalability especially in the local-global index sync part and measure how many nodes it can handle while maintaining optimal performance. Performance will be shown in graphs or charts for visualization.

## Bibliographies
[1] Khudhur, N., & Fujita, S. (2019, November). Siva-The IPFS search engine. In 2019 Seventh International Symposium on Computing and Networking (CANDAR) (pp. 150-156). IEEE.
[2] IPFS-Search. (n.d.). IPFS-Search documentation. Read the Docs. https://ipfs-search.readthedocs.io/en/latest/index.html
[3] Haasdijk, J. (2018). Searching IPFS: A study on how to search files in a peer-to-peer network. Radboud University Nijmegen. Retrieved from https://www.cs.ru.nl/bachelors-theses/2018/Jasper_Haasdijk___4449754___Searching_ IPFS.pdf
[4] Wang, F., & Wu, Y. (2020, December). Keyword Search Technology in Content Addressable Storage System. In 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS) (pp. 728-735). IEEE.